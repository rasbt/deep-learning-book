{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.7.1\n",
      "IPython 7.2.0\n",
      "\n",
      "torch 1.0.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Runs on CPU or GPU (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -Standardizing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example for working with standardized images, that is, images where the image pixels in each image has mean zero and unit variance across the channel. \n",
    "\n",
    "The general equation for z-score standardization is computed as \n",
    "\n",
    "$$x' = \\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation of the training set, respectively. Then $x_i'$ is the scaled feature feature value, and $x_i$ is the original feature value.\n",
    "\n",
    "I.e, for grayscale images, we would obtain 1 mean and 1 standard deviation. For RGB images (3 color channels), we would obtain 3 mean values and 3 standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Mean and Standard Deviation for Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to determine the mean and standard deviation for each color channel in the training set. Since we assume the entire dataset does not fit into the computer memory all at once, we do this in an incremental fashion, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.1307])\n",
      "Std Dev: tensor([0.3077])\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### PRELIMINARY DATALOADER\n",
    "##############################\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False)\n",
    "\n",
    "train_mean = []\n",
    "train_std = []\n",
    "\n",
    "for i, image in enumerate(train_loader, 0):\n",
    "    numpy_image = image[0].numpy()\n",
    "    \n",
    "    batch_mean = np.mean(numpy_image, axis=(0, 2, 3))\n",
    "    batch_std = np.std(numpy_image, axis=(0, 2, 3))\n",
    "    \n",
    "    train_mean.append(batch_mean)\n",
    "    train_std.append(batch_std)\n",
    "\n",
    "train_mean = torch.tensor(np.mean(train_mean, axis=0))\n",
    "train_std = torch.tensor(np.mean(train_std, axis=0))\n",
    "\n",
    "print('Mean:', train_mean)\n",
    "print('Std Dev:', train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that**\n",
    "\n",
    "- For RGB images (3 color channels), we would get 3 means and 3 standard deviations.\n",
    "- The transforms.ToTensor() method converts images to [0, 1] range, which is why the mean and standard deviation values are below 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized Dataset Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use a custom transform function to standardize the dataset according the the mean and standard deviation we computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=train_mean, std=train_std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=custom_transform,\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data', \n",
    "                              train=False, \n",
    "                              transform=custom_transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dataset can be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the given batch, check that the channel means and standard deviations are roughly 0 and 1, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel mean: tensor(0.0043)\n",
      "Channel std: tensor(1.0041)\n"
     ]
    }
   ],
   "source": [
    "print('Channel mean:', torch.mean(images[:, 0, :, :]))\n",
    "print('Channel std:', torch.std(images[:, 0, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "        \n",
    "        # 28x28x1 => 28x28x4\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1,\n",
    "                                      out_channels=4,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1) # (1(28-1) - 28 + 3) / 2 = 1\n",
    "        # 28x28x4 => 14x14x4\n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0) # (2(14-1) - 28 + 2) = 0                                       \n",
    "        # 14x14x4 => 14x14x8\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=4,\n",
    "                                      out_channels=8,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1) # (1(14-1) - 14 + 3) / 2 = 1                 \n",
    "        # 14x14x8 => 7x7x8                             \n",
    "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0) # (2(7-1) - 14 + 2) = 0\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(7*7*8, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_1(out)\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        \n",
    "        logits = self.linear_1(out.view(-1, 7*7*8))\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "    \n",
    "torch.manual_seed(random_seed)\n",
    "model = ConvNet(num_classes=num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/469 | Cost: 2.3170\n",
      "Epoch: 001/010 | Batch 050/469 | Cost: 0.9476\n",
      "Epoch: 001/010 | Batch 100/469 | Cost: 0.4006\n",
      "Epoch: 001/010 | Batch 150/469 | Cost: 0.2662\n",
      "Epoch: 001/010 | Batch 200/469 | Cost: 0.3217\n",
      "Epoch: 001/010 | Batch 250/469 | Cost: 0.2300\n",
      "Epoch: 001/010 | Batch 300/469 | Cost: 0.1494\n",
      "Epoch: 001/010 | Batch 350/469 | Cost: 0.1837\n",
      "Epoch: 001/010 | Batch 400/469 | Cost: 0.2072\n",
      "Epoch: 001/010 | Batch 450/469 | Cost: 0.1540\n",
      "Epoch: 001/010 training accuracy: 95.70%\n",
      "Epoch: 002/010 | Batch 000/469 | Cost: 0.1078\n",
      "Epoch: 002/010 | Batch 050/469 | Cost: 0.1393\n",
      "Epoch: 002/010 | Batch 100/469 | Cost: 0.1594\n",
      "Epoch: 002/010 | Batch 150/469 | Cost: 0.2173\n",
      "Epoch: 002/010 | Batch 200/469 | Cost: 0.1095\n",
      "Epoch: 002/010 | Batch 250/469 | Cost: 0.1320\n",
      "Epoch: 002/010 | Batch 300/469 | Cost: 0.1019\n",
      "Epoch: 002/010 | Batch 350/469 | Cost: 0.1103\n",
      "Epoch: 002/010 | Batch 400/469 | Cost: 0.1028\n",
      "Epoch: 002/010 | Batch 450/469 | Cost: 0.0676\n",
      "Epoch: 002/010 training accuracy: 96.50%\n",
      "Epoch: 003/010 | Batch 000/469 | Cost: 0.0711\n",
      "Epoch: 003/010 | Batch 050/469 | Cost: 0.1319\n",
      "Epoch: 003/010 | Batch 100/469 | Cost: 0.1434\n",
      "Epoch: 003/010 | Batch 150/469 | Cost: 0.0449\n",
      "Epoch: 003/010 | Batch 200/469 | Cost: 0.0785\n",
      "Epoch: 003/010 | Batch 250/469 | Cost: 0.2009\n",
      "Epoch: 003/010 | Batch 300/469 | Cost: 0.1132\n",
      "Epoch: 003/010 | Batch 350/469 | Cost: 0.0919\n",
      "Epoch: 003/010 | Batch 400/469 | Cost: 0.0851\n",
      "Epoch: 003/010 | Batch 450/469 | Cost: 0.1058\n",
      "Epoch: 003/010 training accuracy: 96.23%\n",
      "Epoch: 004/010 | Batch 000/469 | Cost: 0.1385\n",
      "Epoch: 004/010 | Batch 050/469 | Cost: 0.1208\n",
      "Epoch: 004/010 | Batch 100/469 | Cost: 0.0961\n",
      "Epoch: 004/010 | Batch 150/469 | Cost: 0.1299\n",
      "Epoch: 004/010 | Batch 200/469 | Cost: 0.0981\n",
      "Epoch: 004/010 | Batch 250/469 | Cost: 0.0509\n",
      "Epoch: 004/010 | Batch 300/469 | Cost: 0.0796\n",
      "Epoch: 004/010 | Batch 350/469 | Cost: 0.0972\n",
      "Epoch: 004/010 | Batch 400/469 | Cost: 0.2142\n",
      "Epoch: 004/010 | Batch 450/469 | Cost: 0.0949\n",
      "Epoch: 004/010 training accuracy: 97.50%\n",
      "Epoch: 005/010 | Batch 000/469 | Cost: 0.0960\n",
      "Epoch: 005/010 | Batch 050/469 | Cost: 0.0131\n",
      "Epoch: 005/010 | Batch 100/469 | Cost: 0.1011\n",
      "Epoch: 005/010 | Batch 150/469 | Cost: 0.0436\n",
      "Epoch: 005/010 | Batch 200/469 | Cost: 0.0388\n",
      "Epoch: 005/010 | Batch 250/469 | Cost: 0.0458\n",
      "Epoch: 005/010 | Batch 300/469 | Cost: 0.1060\n",
      "Epoch: 005/010 | Batch 350/469 | Cost: 0.0974\n",
      "Epoch: 005/010 | Batch 400/469 | Cost: 0.0916\n",
      "Epoch: 005/010 | Batch 450/469 | Cost: 0.0629\n",
      "Epoch: 005/010 training accuracy: 97.83%\n",
      "Epoch: 006/010 | Batch 000/469 | Cost: 0.0767\n",
      "Epoch: 006/010 | Batch 050/469 | Cost: 0.0243\n",
      "Epoch: 006/010 | Batch 100/469 | Cost: 0.0376\n",
      "Epoch: 006/010 | Batch 150/469 | Cost: 0.0463\n",
      "Epoch: 006/010 | Batch 200/469 | Cost: 0.1139\n",
      "Epoch: 006/010 | Batch 250/469 | Cost: 0.0466\n",
      "Epoch: 006/010 | Batch 300/469 | Cost: 0.0167\n",
      "Epoch: 006/010 | Batch 350/469 | Cost: 0.0145\n",
      "Epoch: 006/010 | Batch 400/469 | Cost: 0.0622\n",
      "Epoch: 006/010 | Batch 450/469 | Cost: 0.0570\n",
      "Epoch: 006/010 training accuracy: 97.94%\n",
      "Epoch: 007/010 | Batch 000/469 | Cost: 0.1525\n",
      "Epoch: 007/010 | Batch 050/469 | Cost: 0.0280\n",
      "Epoch: 007/010 | Batch 100/469 | Cost: 0.0461\n",
      "Epoch: 007/010 | Batch 150/469 | Cost: 0.0617\n",
      "Epoch: 007/010 | Batch 200/469 | Cost: 0.0640\n",
      "Epoch: 007/010 | Batch 250/469 | Cost: 0.0719\n",
      "Epoch: 007/010 | Batch 300/469 | Cost: 0.0098\n",
      "Epoch: 007/010 | Batch 350/469 | Cost: 0.0853\n",
      "Epoch: 007/010 | Batch 400/469 | Cost: 0.0961\n",
      "Epoch: 007/010 | Batch 450/469 | Cost: 0.0634\n",
      "Epoch: 007/010 training accuracy: 98.16%\n",
      "Epoch: 008/010 | Batch 000/469 | Cost: 0.1086\n",
      "Epoch: 008/010 | Batch 050/469 | Cost: 0.0962\n",
      "Epoch: 008/010 | Batch 100/469 | Cost: 0.1354\n",
      "Epoch: 008/010 | Batch 150/469 | Cost: 0.0199\n",
      "Epoch: 008/010 | Batch 200/469 | Cost: 0.0718\n",
      "Epoch: 008/010 | Batch 250/469 | Cost: 0.0504\n",
      "Epoch: 008/010 | Batch 300/469 | Cost: 0.0830\n",
      "Epoch: 008/010 | Batch 350/469 | Cost: 0.0869\n",
      "Epoch: 008/010 | Batch 400/469 | Cost: 0.0884\n",
      "Epoch: 008/010 | Batch 450/469 | Cost: 0.1108\n",
      "Epoch: 008/010 training accuracy: 97.97%\n",
      "Epoch: 009/010 | Batch 000/469 | Cost: 0.0664\n",
      "Epoch: 009/010 | Batch 050/469 | Cost: 0.0787\n",
      "Epoch: 009/010 | Batch 100/469 | Cost: 0.1523\n",
      "Epoch: 009/010 | Batch 150/469 | Cost: 0.0497\n",
      "Epoch: 009/010 | Batch 200/469 | Cost: 0.0618\n",
      "Epoch: 009/010 | Batch 250/469 | Cost: 0.1518\n",
      "Epoch: 009/010 | Batch 300/469 | Cost: 0.0483\n",
      "Epoch: 009/010 | Batch 350/469 | Cost: 0.0393\n",
      "Epoch: 009/010 | Batch 400/469 | Cost: 0.0292\n",
      "Epoch: 009/010 | Batch 450/469 | Cost: 0.0182\n",
      "Epoch: 009/010 training accuracy: 98.24%\n",
      "Epoch: 010/010 | Batch 000/469 | Cost: 0.0119\n",
      "Epoch: 010/010 | Batch 050/469 | Cost: 0.0584\n",
      "Epoch: 010/010 | Batch 100/469 | Cost: 0.0239\n",
      "Epoch: 010/010 | Batch 150/469 | Cost: 0.0253\n",
      "Epoch: 010/010 | Batch 200/469 | Cost: 0.0235\n",
      "Epoch: 010/010 | Batch 250/469 | Cost: 0.0453\n",
      "Epoch: 010/010 | Batch 300/469 | Cost: 0.0753\n",
      "Epoch: 010/010 | Batch 350/469 | Cost: 0.1533\n",
      "Epoch: 010/010 | Batch 400/469 | Cost: 0.0182\n",
      "Epoch: 010/010 | Batch 450/469 | Cost: 0.0618\n",
      "Epoch: 010/010 training accuracy: 98.17%\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for features, targets in data_loader:\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "    \n",
    "    model = model.eval()\n",
    "    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "          epoch+1, num_epochs, \n",
    "          compute_accuracy(model, train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.03%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy       1.15.4\n",
      "torch       1.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
