{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by Sebastian Raschka. All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "\n",
    "\n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix G - PyTorch Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [PyTorch in a Nutshell](#PyTorch-in-a-Nutshell)\n",
    "* [Tensors](#Tensors)\n",
    "* [Installation](#Installation)\n",
    "* [CPU and GPU](#CPU-and-GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appendix offers a brief overview of [*PyTorch*](http://pytorch.org), an open-source library for numerical computation and deep learning. This section is intended for readers who want to gain a basic overview of this library before progressing through the hands-on sections that are concluding the main chapters.\n",
    "The majority of hands-on sections in this book focus will use PyTorch, assuming that you have PyTorch >=0.1 installed if you are planning to execute the code sections shown in this book.\n",
    "In addition to glancing over this appendix, I recommend the following resources from the official documentation for a more in-depth coverage on using this library:\n",
    "\n",
    "- [API documentation](http://pytorch.org/docs/)\n",
    "- [Example projects](https://github.com/pytorch/examples)\n",
    "- [Tutorials](https://github.com/pytorch/tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, PyTorch is a library for efficient tensor operations with primitives for deep learning, which make the latter more efficient and convenient in contrast to standard linear algebra libraries that only execute operations on Central Processing Units (CPUs). If you have used [NumPy](http://www.numpy.org) before -- Python's main library for scientific computing -- PyTorch may look very familiar to you. However, the main strength of PyTorch is that it allows us to utilize Graphics Processing Units (GPUs). You may have already heard of GPUs; we can think of them as small computer clusters, real power horses,  inside our computers. And compared to state-of-the-art CPUs, GPUs are relatively cheap.\n",
    "\n",
    "In a nutshell, PyTorch is a library for tensor computations, just like NumPy, but with massive GPU acceleration. And one advantage of PyTorch over other existing frameworks for deep learning is that it puts Python first. Thus, the API feels very familiar to Python and NumPy users, and everything happens dynamically without having to set up static computation graphs. (Note that if you don't have a GPU that supports CUDA, PyTorch runs on CPUs as well; the use of a GPU, or multiple GPUs, is entirely optional.)\n",
    "\n",
    "Besides being efficient at performing highly parallelized numerical computations, PyTorch leverages reverse-mode auto-differentiation to compute the derivatives of mathematical functions very efficiently, functions that take multiple inputs and produce a single output. As the developers state it in the documentation, PyTorch is\n",
    "\n",
    "> [...] one of the fastest implementations of [reverse-mode auto-differentiation] to date. You get the best of speed and flexibility for your crazy research.\n",
    "\n",
    "And besides being fast at training small or large neural networks, its elegant API makes neural networks implemented in PyTorch very easy to read and comprehend, which is a huge plus when it comes to academic research and the development of real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction, PyTorch mainly operates on tensors, but what is a *tensor*? In simple terms, we can think of tensors as multi-dimensional arrays of numbers, a generalization of scalars, vectors, and matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scalar: $\\mathbb{R}$\n",
    "2. Vector: $\\mathbb{R}^n$\n",
    "3. Matrix: $\\mathbb{R}^n \\times \\mathbb{R}^m$\n",
    "4. 3-Tensor: $\\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p$\n",
    "5. ...\n",
    "\n",
    "When we describe a tensor, we refer to its \"dimensions\" as the *rank* (or *order*) of a tensor, which is not to be confused with the dimensions of a matrix. For instance, an $m \\times n$ matrix, where $m$ is the number of rows and $n$ is the number of columns, would be a special case of a rank-2 tensor. A visual explanation of tensors and their ranks is given is the figure below.\n",
    "\n",
    "![Tensors](images/appendix_pytorch/tensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code conventions in this book follow the Python 3.x syntax, and while the code examples should be backwards compatible to Python 2.7, I highly recommend the use of Python >=3.5.\n",
    "\n",
    "Once you have your Python environment set up ([Appendix - Python Setup](#python-setup)), the two probably most convenient ways for installing PyTorch are via `pip` or `conda` -- the latter only applies if you have the [Anaconda/Miniconda Python distribution](https://www.continuum.io/downloads) installed, which I prefer and recommend.\n",
    "\n",
    "Since PyTorch is under active development, I recommend you to visit the [PyTorch Website](http://pytorch.org) for more information on the installation procedure that fits your hardware and operating system, for example, as shown in the screenshot below:\n",
    "\n",
    "![Tensors](images/appendix_pytorch/pytorch-install.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (2, 3)\n",
      "Tensor contents:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array([[1., 2., 3.],\n",
    "              [4., 5., 6.]])\n",
    "\n",
    "print('Dimensions:', y.shape)\n",
    "print('Tensor contents:')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: torch.Size([2, 3])\n",
      "Tensor contents:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  2  3\n",
       " 4  5  6\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor([[1., 2., 3.],\n",
    "                  [4., 5., 6.]])\n",
    "\n",
    "print('Dimensions:', x.size())\n",
    "print('Tensor contents:')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.   4.   6.]\n",
      " [  8.  10.  12.]]\n",
      "\n",
      "  2   4   6\n",
      "  8  10  12\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y + y)\n",
    "print(x + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  2  3\n",
       " 4  5  6\n",
       "[torch.DoubleTensor of size 2x3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "y = np.array([[1., 2., 3.],\n",
    "              [4., 5., 6.]])\n",
    "\n",
    "torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  4\n",
       " 2  5\n",
       " 3  6\n",
       "[torch.FloatTensor of size 3x2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  2  3\n",
       " 4  5  6\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.],\n",
       "       [ 2.,  5.],\n",
       "       [ 3.,  6.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "x.creator: None\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print('x:\\n', x)\n",
    "print('x.creator:', x.creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) z:\n",
      " Variable containing:\n",
      " 8  8\n",
      " 8  8\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "2) z.creator: <torch.autograd._functions.basic_ops.PowConstant object at 0x10a6a6508>\n",
      "3) z.requires_grad: True\n",
      "4) z.grad:\n",
      " Variable containing:\n",
      " 0  0\n",
      " 0  0\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = (2*x)**3\n",
    "print('1) z:\\n', z)\n",
    "print('2) z.creator:', z.creator)\n",
    "print('3) z.requires_grad:', z.requires_grad)\n",
    "print('4) z.grad:\\n', z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) out: Variable containing:\n",
      " 32\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "2) out.creator <torch.autograd._functions.reduce.Sum object at 0x10a6a65a0>\n"
     ]
    }
   ],
   "source": [
    "out = torch.sum(z)\n",
    "print('1) out:', out)\n",
    "print('2) out.creator', out.creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 24  24\n",
       " 24  24\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter 78\n",
      "0.0005492811324074864\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "\n",
    "x = Variable(torch.Tensor([[1.0], \n",
    "                           [2.1], \n",
    "                           [3.6], \n",
    "                           [4.2], \n",
    "                           [6.0], \n",
    "                           [7.0]]))\n",
    "\n",
    "y = Variable(torch.Tensor([1.0, 2.1, 3.6, 4.2, 6.0, 7.0]))\n",
    "\n",
    "weights = Variable(torch.zeros(1, 1), requires_grad=True)\n",
    "bias = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "for i in range(5000):\n",
    "\n",
    "    net_input = x.mm(weights)\n",
    "    net_input += bias.unsqueeze(0).expand_as(net_input)\n",
    "    # net_input = F.linear(input, weights, bias)\n",
    "    \n",
    "    loss = torch.mean((net_input - y)**2)\n",
    "    loss.backward()\n",
    "    weights.data.add_(-0.00001 * weights.grad.data)\n",
    "    bias.data.add_(-0.00001 * loss.data)\n",
    "    \n",
    "    if loss.data[0] < 1e-3:\n",
    "        break\n",
    "\n",
    "print('n_iter', i)\n",
    "print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter 78\n",
      "0.0005492811324074864\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "\n",
    "x = Variable(torch.Tensor([[1.0], \n",
    "                           [2.1], \n",
    "                           [3.6], \n",
    "                           [4.2], \n",
    "                           [6.0], \n",
    "                           [7.0]]))\n",
    "\n",
    "y = Variable(torch.Tensor([1.0, 2.1, 3.6, 4.2, 6.0, 7.0]))\n",
    "\n",
    "weights = Variable(torch.zeros(1, 1), requires_grad=True)\n",
    "bias = Variable(torch.zeros(1))\n",
    "\n",
    "for i in range(5000):\n",
    "\n",
    "    net_input = F.linear(x, weights, bias)\n",
    "    \n",
    "    loss = torch.mean((net_input - y)**2)\n",
    "    loss.backward()\n",
    "    weights.data.add_(-0.00001 * weights.grad.data)\n",
    "    bias.data.add_(-0.00001 * loss.data)\n",
    "    \n",
    "    if loss.data[0] < 1e-3:\n",
    "        break\n",
    "\n",
    "print('n_iter', i)\n",
    "print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter 222\n",
      "0.0002557723200879991\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "\n",
    "x = Variable(torch.Tensor([[1.0], \n",
    "                           [2.1], \n",
    "                           [3.6], \n",
    "                           [4.2], \n",
    "                           [6.0], \n",
    "                           [7.0]]))\n",
    "\n",
    "y = Variable(torch.Tensor([1.0, 2.1, 3.6, 4.2, 6.0, 7.0]))\n",
    "\n",
    "weights = Variable(torch.zeros(1, 1), requires_grad=True)\n",
    "bias = Variable(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "for i in range(5000):\n",
    "\n",
    "    net_input = F.linear(x, weights)\n",
    "    \n",
    "    loss = torch.mean((net_input - y)**2)\n",
    "    loss.backward()\n",
    "    weights.data.add_(-0.0001 * weights.grad.data)\n",
    "    #bias.data.add_(-0.00001 * loss.data)\n",
    "    \n",
    "    if loss.data[0] < 1e-3:\n",
    "        break\n",
    "\n",
    "print('n_iter', i)\n",
    "print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004064568784087896\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = torch.nn.Linear(1, 1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "model = Model()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "    \n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()        \n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  0.9725\n",
       " [torch.FloatTensor of size 1x1], Parameter containing:\n",
       "  0.1380\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
